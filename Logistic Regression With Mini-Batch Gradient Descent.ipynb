{"cells":[{"cell_type":"markdown","id":"73650a3d-5f06-4713-a90b-0fcba4a91a9f","metadata":{},"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkCV0101ENCoursera872-2023-01-01\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","id":"8759d682-b5d6-46ad-8bb4-c2823194450b","metadata":{},"source":["<h1>Logistic Regression With Mini-Batch Gradient Descent </h1> \n"]},{"cell_type":"markdown","id":"8d6d7061-ae64-4821-a442-699b37ef1cf1","metadata":{},"source":["<h2>Objective</h2>\n","\n","<ul>\n","    <li>Represent your data as a Dataset object</li>\n","    <li>Create a Logistic Regression Model using PyTorch</li>\n","    <li>Set a Criterion to calculate Loss</li>\n","    <li>Create a Data Loader and set the Batch Size</li>\n","    <li>Create an Optimizer to update Model Parameters and set Learning Rate</li>\n","    <li>Train a Model</li>\n","</ul> \n"]},{"attachments":{},"cell_type":"markdown","id":"76e2ee5f-cc71-4635-8eed-f5b67c1f655e","metadata":{},"source":["<h2>Table of Contents</h2>\n","<p>In this lab, you will learn how to train a PyTorch Logistic Regression model using Mini-Batch Gradient Descent.</p>\n","\n","<ul>\n","    <li>Load Data</li>\n","    <li>Create the Model and Total Loss Function (Cost)</li>\n","    <li>Setting the Batch Size using a Data Loader</li>\n","    <li>Setting the Learning Rate</li>\n","    <li>Train the Model via Mini-Batch Gradient Descent</li>\n","    <li>Question</li>\n","</ul>\n","\n","\n","<hr>\n"]},{"cell_type":"markdown","id":"fe07883a-a3b8-486d-8374-4e3042c68efd","metadata":{},"source":["<h2>Preparation</h2>\n"]},{"cell_type":"code","execution_count":null,"id":"cc5f6312-65fa-4596-997f-99d9f8a656d6","metadata":{},"outputs":[],"source":["!pip3 install torch torchvision torchaudio"]},{"cell_type":"code","execution_count":1,"id":"00198e51-1e75-49f6-bd10-15df5e771734","metadata":{},"outputs":[],"source":["# Import the libraries we need for this lab\n","\n","# Allows us to use arrays to manipulate and store data\n","import numpy as np\n","# Used to graph data and loss curves\n","import matplotlib.pyplot as plt\n","from mpl_toolkits import mplot3d\n","# PyTorch Library\n","import torch\n","# Used to help create the dataset and perform mini-batch\n","from torch.utils.data import Dataset, DataLoader\n","# PyTorch Neural Network\n","import torch.nn as nn"]},{"attachments":{},"cell_type":"markdown","id":"db62fae6-3d85-4de0-acbb-654d2414e474","metadata":{},"source":["\n","\n","\n","Equation of linear classifier : <code>Z = wx+b </code>. x is input feature vector.The linear classifier assigns a class label to an input based on the sign of the predicted output. If Z is positive, the input is classified into one class, and if Z is negative, it is classified into another class. \n","\n","\n","------------------\n","\n","for x=2, plot the  decision plane Z = wx+b . The line  where the decision plane intersects the x-y plane is the decision boundary. anything on the left is one class and on the right is another class.\n","\n","----------\n","\n","Well it is not guaranted that decision boundary is a linear. So we need a threshold function to what extent our result is guaranted.Threshold function is  sigma(z) = 1/(1+ e^-z) If the value of sigmoid function is greater than 0.5 ,is one class and if less then another class.\n","\n","-------\n","\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We can use loss function and cost function to determine the best parameters. A loss function tells you how good our predictions is. Whenever it predicts wrong class ,loss is 1 and when correct loss is 0 . cost is the summation of all the losses. So cost is a function of learnable parameters.\n","\n","-------------\n","\n","The cross-entropy loss  calculated as follows:\n","\n","L(y, ŷ) = - (y * log(ŷ) + (1 - y) * log(1 - ŷ))\n","\n","where:\n","\n","    L(y, ŷ) is the cross-entropy loss,\n","    y is the true label (0 or 1),\n","    ŷ is the predicted probability of the positive class (between 0 and 1).\n","\n","The loss function penalizes the model more when it makes incorrect predictions with high confidence. If the true label (y) is 1, the first term in the loss function penalizes the model when the predicted probability (ŷ) is low. If the true label is 0, the second term penalizes the model when the predicted probability is high.\n","\n","During training, the goal is to minimize the average cross-entropy loss over the entire training dataset. This is typically done using optimization algorithms such as gradient descent, which adjust the model's weights and biases to find the optimal values that minimize the loss."]},{"cell_type":"code","execution_count":34,"id":"4dbe4595-5594-4495-bd6c-413fce6f7d0f","metadata":{},"outputs":[],"source":["# Create plot_error_surfaces class ,child of object class .In this lab we use logistic regression of input feature 1 so w is a  value and b is also a value.\n","\n","class plot_error_surfaces(object):\n","    \n","    # Construstor\n","    def __init__(self, w_range, b_range, X, Y, n_samples = 30, go = True):\n","        W = np.linspace(-w_range, w_range, n_samples)     #n_samples equally spaced points in that w_range\n","        B = np.linspace(-b_range, b_range, n_samples)\n","        \n","        w, b = np.meshgrid(W, B)    # possible combinations of W and B. each are 30X30 array.\n","        #print(w) \n","        #print(b)\n","        \n","        Z = np.zeros((30, 30))     # to store average loss for each combinations of w and b.\n","        count1 = 0\n","        #X is the input vector and y is the output vector .\n","        self.y = Y.numpy()  \n","        self.x = X.numpy()\n","        for w1, b1 in zip(w, b):     # w1 and b1 are array of size 30 each .w1 is W itself while b1 is the 30X1  array of one value of B.\n","             \n","            count2 = 0\n","            for w2, b2 in zip(w1, b1):\n","            \n","                 \n","                yhat= 1 / (1 + np.exp(-1*(w2*self.x+b2)))   # self.x is a vector .yhat is sigmoid of z.\n","                \n","                #print(w2*self.x+b2)              #it is also a array.\n","                #print(yhat)                        #yhat is also a array of yhat value of corresponding inputs.\n","                \n","                Z[count1,count2]=-1*np.mean(self.y*np.log(yhat+1e-16) +(1-self.y)*np.log(1-yhat+1e-16))    # cross entropy loss for that pair of w2 and b2.\n","                count2 += 1   \n","            count1 += 1\n","        self.Z = Z\n","        self.w = w\n","        self.b = b\n","        self.W = []\n","        self.B = []\n","        self.LOSS = []\n","        self.n = 0\n","        if go == True:\n","            plt.figure()\n","            plt.figure(figsize=(7.5, 5))\n","            plt.axes(projection='3d').plot_surface(self.w, self.b, self.Z, rstride=1, cstride=1, cmap='viridis', edgecolor='none')\n","            plt.title('Loss Surface')\n","            plt.xlabel('w')\n","            plt.ylabel('b')\n","            plt.show()\n","            plt.figure()\n","            plt.title('Loss Surface Contour')\n","            plt.xlabel('w')\n","            plt.ylabel('b')\n","            plt.contour(self.w, self.b, self.Z)\n","            plt.show()\n","            \n","     # Setter\n","     #This function, set_para_loss, is used to store the values of model parameters (W and B) and the corresponding loss (LOSS) during training.\n","    def set_para_loss(self, model, loss):\n","        self.n = self.n + 1     # no of iterations \n","        #print(self.n)\n","        self.W.append(list(model.parameters())[0].item())  # corresponding w and b value.\n","        self.B.append(list(model.parameters())[1].item())\n","        #print(list(model.parameters())[0].item())\n","        #print(list(model.parameters())[1].item())\n","        self.LOSS.append(loss)\n","    \n","    # Plot diagram\n","    def final_plot(self): \n","        ax = plt.axes(projection='3d')\n","        ax.plot_wireframe(self.w, self.b, self.Z)\n","        ax.scatter(self.W, self.B, self.LOSS, c='r', marker='x', s=200, alpha=1)\n","        plt.figure()\n","        plt.contour(self.w, self.b, self.Z)\n","        plt.scatter(self.W, self.B, c='r', marker='x')\n","        plt.xlabel('w')\n","        plt.ylabel('b')\n","        plt.show()\n","        \n","    # Plot diagram\n","    def plot_ps(self):\n","        plt.subplot(121)\n","        plt.ylim\n","        plt.plot(self.x[self.y==0], self.y[self.y==0], 'ro', label=\"training points\")\n","        plt.plot(self.x[self.y==1], self.y[self.y==1]-1, 'o', label=\"training points\")\n","        plt.plot(self.x, self.W[-1] * self.x + self.B[-1], label=\"estimated line\")\n","        plt.xlabel('x')\n","        plt.ylabel('y')\n","        plt.ylim((-0.1, 2))\n","        plt.title('Data Space Iteration:' + str(self.n))\n","        plt.show()\n","        plt.subplot(122)\n","        plt.contour(self.w, self.b, self.Z)\n","        plt.scatter(self.W, self.B, c='r', marker='x')\n","        plt.title('Loss Surface Contour Iteration' + str(self.n))\n","        plt.xlabel('w')\n","        plt.ylabel('b')\n","        \n","# Plot the diagram\n","\n","def PlotStuff(X, Y, model, epoch, leg=True):\n","    \n","    plt.plot(X.numpy(), model(X).detach().numpy(), label=('epoch ' + str(epoch)))\n","    plt.plot(X.numpy(), Y.numpy(), 'r')\n","    if leg == True:\n","        plt.legend()\n","    else:\n","        pass"]},{"cell_type":"markdown","id":"49f6b5c7-69bd-49c0-ad50-89313d03f34b","metadata":{},"source":["Set the random seed:\n"]},{"cell_type":"code","execution_count":3,"id":"005d6eca-442c-4ca8-9873-e117b2d6a53c","metadata":{},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x7fbdd26dc270>"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Setting the seed will allow us to control randomness and give us reproducibility\n","torch.manual_seed(0)"]},{"cell_type":"markdown","id":"c29296ac-1bbd-4a75-ac3c-ac7847053d84","metadata":{},"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","id":"5b7d2526-f662-43c8-bb72-983869793151","metadata":{},"source":["<h2 id=\"Makeup_Data\">Load Data</h2>\n"]},{"cell_type":"markdown","id":"1751a044-0f79-4151-b981-acea1a9ec457","metadata":{},"source":["The Dataset class represents a dataset. Your custom dataset should inherit Dataset which we imported above and override the following methods:\n","\n","<p><code>__len__</code> so that len(dataset) returns the size of the dataset.</p>\n","<p><code>__getitem__</code> to support the indexing such that dataset[i] can be used to get ith sample</p>\n","\n","Below we will create a sample dataset\n"]},{"cell_type":"code","execution_count":4,"id":"1beb505c-36a6-46c5-b5ac-a65291eb990f","metadata":{},"outputs":[],"source":["# Create the custom Data class which inherits Dataset as parent class.\n","class Data(Dataset):\n","    \n","    # Constructor\n","    def __init__(self):\n","        # Create X values from -1 to 1 with step .1\n","        self.x = torch.arange(-1, 1, 0.1).view(-1, 1)  # 20X1 tensor with 1(arguement as 1 given) column.\n","        # Create Y values all set to 0\n","        self.y = torch.zeros(self.x.shape[0], 1)\n","        # Set the X values above 0.2 to 1\n","        self.y[self.x[:, 0] > 0.2] = 1    #This line sets the values of self.y to 1 where the corresponding values in self.x (first column) are greater than 0.2.\n","        # Set the .len attribute because we need to override the __len__ method\n","        self.len = self.x.shape[0]\n","        #print(self.x.shape[1])\n","    \n","    # Getter that returns the data at the given index\n","    def __getitem__(self, index):      \n","        return self.x[index], self.y[index]\n","    \n","    # Get length of the dataset\n","    def __len__(self):\n","        return self.len"]},{"cell_type":"markdown","id":"ba54935b-64a3-4fc9-9678-93a9c89533ef","metadata":{},"source":["Make <code>Data</code> object\n"]},{"cell_type":"code","execution_count":7,"id":"9d9a9515-001c-42e7-8959-f8bb16d94f9d","metadata":{},"outputs":[],"source":["# data_set is a Data object\n","data_set = Data()\n"]},{"cell_type":"markdown","id":"66dc85de-2f1c-4a7d-929c-70c18993f6b9","metadata":{},"source":["We can see the X values of the dataset\n"]},{"cell_type":"code","execution_count":null,"id":"46a9c56d-803a-4f31-a3fc-4d330102261c","metadata":{},"outputs":[],"source":["data_set.x"]},{"cell_type":"markdown","id":"ec6d5def-4cc4-465c-a3e3-89e71a863731","metadata":{},"source":["We can see the Y values of the dataset which correspond to the class of the X value\n"]},{"cell_type":"code","execution_count":null,"id":"ed00e666-3604-4623-8ceb-ead666a6d443","metadata":{},"outputs":[],"source":["data_set.y"]},{"cell_type":"markdown","id":"9177355c-a3b9-4fbf-b667-912189432d56","metadata":{},"source":["We can get the length of the dataset\n"]},{"cell_type":"code","execution_count":18,"id":"722441a1-523a-48e8-9e4b-3a1919682e65","metadata":{},"outputs":[{"data":{"text/plain":["20"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["len(data_set)"]},{"cell_type":"markdown","id":"17c9f55d-cf95-4f57-92b3-ac801056a7b8","metadata":{},"source":["We can get the label $y$ as well as the $x$ for the first sample \n"]},{"cell_type":"code","execution_count":19,"id":"d34d9878-6336-497f-9cb1-49c8425f97e1","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["x = tensor([-1.]),  y = tensor([0.])\n"]}],"source":["x,y = data_set[0]  #data_set is 20X2 tensor.\n","print(\"x = {},  y = {}\".format(x,y))"]},{"cell_type":"markdown","id":"4b0c57f1-e84f-4483-8d0b-03f073dfe31c","metadata":{},"source":["We can get the label $y$ as well as the $x$ for the second sample:\n"]},{"cell_type":"code","execution_count":null,"id":"be9d07fc-1a5c-46f7-8125-6c7ca5d1ccae","metadata":{},"outputs":[],"source":["x,y = data_set[1]\n","print(\"x = {},  y = {}\".format(x,y))"]},{"cell_type":"markdown","id":"b541e4e0-e174-4bca-8277-2070ab58ef67","metadata":{},"source":[" We can see we can separate the one-dimensional dataset into two classes:\n"]},{"cell_type":"code","execution_count":null,"id":"b144b7f8-c44f-49a4-9bec-b2f61f1b5be9","metadata":{},"outputs":[],"source":["plt.plot(data_set.x[data_set.y==0], data_set.y[data_set.y==0], 'ro', label=\"y=0\")\n","plt.plot(data_set.x[data_set.y==1], data_set.y[data_set.y==1]-1, 'o', label=\"y=1\")\n","plt.xlabel('x')\n","plt.ylabel('y')\n","plt.legend()          "]},{"cell_type":"markdown","id":"a06fcabe-08c6-4bf5-bbee-e7c64fc49063","metadata":{},"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","id":"bd776a25-f233-41da-b3b4-ef259c76bb39","metadata":{},"source":["<h2 id=\"Model_Cost\">Create the Model and Total Loss Function (Cost)</h2>\n"]},{"cell_type":"markdown","id":"cc338448-0f6d-429c-86fd-d067966f10c7","metadata":{},"source":["We will create a custom class that defines the architecture of Logistic Regression using PyTorch. Logistic Regression has a single layer where the input is the number of features an X value of the dataset has (dimension of X) and there is a single output. The output of the layer is put into a sigmoid function which is a function between 0 and 1. The larger the output of the layer the closer it is to 1 and the smaller the output is the closer it is to 0. The sigmoid function will allow us to turn this output into a classification problem. If the output value is closer to 1 it is one class if it is closer to 0 it is in another.\n"]},{"cell_type":"code","execution_count":12,"id":"494c2942-e162-4486-80af-9e7c503917e3","metadata":{},"outputs":[],"source":["# Create logistic_regression class using PyTorch that inherits nn.Module which is the base class for all neural networks\n","class logistic_regression(nn.Module):\n","    \n","    # Constructor\n","    def __init__(self, n_inputs):\n","        super(logistic_regression, self).__init__()      #In the constructor method (__init__), the class is initialized. It takes n_inputs as a parameter, which represents the number of input features for the logistic regression model.\n","        #The super() function is called to invoke the constructor of the parent class (nn.Module) and initialize the object.\n","        # Single layer of Logistic Regression with number of inputs being n_inputs and there being 1 output \n","        self.linear = nn.Linear(n_inputs, 1)   #This layer will be responsible for the linear mapping of inputs to the output.\n","        #print(self.linear)\n","        \n","    # Prediction\n","    def forward(self, x):  \n","        yhat = torch.sigmoid(self.linear(x))   # It takes an input tensor x and passes it through the linear layer (self.linear). The output of the linear layer(z value ,whether it is positive or negative) is then passed through the sigmoid activation function using torch.sigmoid(). Finally, the result is returned as the predicted output yhat.\n","        return yhat"]},{"cell_type":"code","execution_count":14,"id":"c15a41cf-df83-42d6-9560-70208a8e74f1","metadata":{},"outputs":[],"source":["# We defined the logistic regression class above . Create a logistic  regression  model with no of input features 1.\n","\n","model = logistic_regression(1)"]},{"cell_type":"markdown","id":"dc3e5ba4-b182-4a33-86de-a80828716f58","metadata":{},"source":["We can make a prediction sigma $\\sigma$ this uses the forward function defined above\n"]},{"cell_type":"code","execution_count":null,"id":"95e74b96-63b9-4e2d-8575-669da4020d67","metadata":{},"outputs":[],"source":["x = torch.tensor([-1.0])\n","#x2 = torch.tensor([5.0])\n","\n","#This line calls the forward method of the model object, passing the input tensor x. The forward method performs the forward pass computation of the model, which includes applying the linear transformation and the sigmoid activation function to the input. The output of the forward method is assigned to the variable sigma, which represents the predicted output.\n","sigma = model(x)        \n","sigma"]},{"cell_type":"markdown","id":"9d123e6e-bce9-437f-b244-89879241a392","metadata":{},"source":["Create a <code>plot_error_surfaces</code> object to visualize the data space and the learnable parameters space during training:\n","\n","We can see on the Loss Surface graph, the loss value varying across w and b values with yellow being high loss and dark blue being low loss which is what we want\n","\n","On the Loss Surface Contour graph we can see a top-down view of the Loss Surface graph\n"]},{"cell_type":"code","execution_count":null,"id":"f3ccda32-71ef-4f5a-bef0-76c6da24433c","metadata":{},"outputs":[],"source":["# Create the plot_error_surfaces object\n","\n","# 15 is the range of w\n","# 13 is the range of b\n","# data_set[:][0] are all the X values\n","# data_set[:][1] are all the Y values\n","#it plots the 3D surface of loss or cost which is a function of  w and b and stores the average loss for each combination of w and b.\n","get_surface = plot_error_surfaces(15, 13, data_set[:][0], data_set[:][1])"]},{"cell_type":"markdown","id":"12eff879-44d0-4ab2-8d5a-a95c1ec641ae","metadata":{},"source":["We define a criterion using Binary Cross Entropy Loss. This will measure the difference/loss between the prediction and actual value.\n"]},{"cell_type":"code","execution_count":21,"id":"08b53940-903b-4b17-971d-b6de83bcd852","metadata":{},"outputs":[],"source":["criterion = nn.BCELoss()    #  creates an instance of the Binary Cross Entropy Loss (BCELoss) in PyTorch. The nn.BCELoss() function is part of the torch.nn module."]},{"cell_type":"markdown","id":"1c4e01ca-4f5d-4833-a726-f5f700f8bce5","metadata":{},"source":["We have our samples:\n"]},{"cell_type":"code","execution_count":22,"id":"f8a6cd45-7381-4391-bdf6-a9389325e432","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["x = tensor([-1.]),  y = tensor([0.])\n"]}],"source":["x, y = data_set[0]\n","print(\"x = {},  y = {}\".format(x,y))"]},{"cell_type":"markdown","id":"0f29c5b8-4a00-437a-8d34-8c590cb1a4e3","metadata":{},"source":["We can make a prediction using the model:\n"]},{"cell_type":"code","execution_count":null,"id":"b7e62200-5c8f-445e-917d-a2c8eeefe69b","metadata":{},"outputs":[],"source":["sigma = model(x)\n","    # probablity that the input belongs to that class 1 or not.\n","loss = criterion(sigma, y)\n","loss"]},{"cell_type":"markdown","id":"e7765cfb-9e17-4baa-b0bd-eaa6e922f2c2","metadata":{},"source":["## Setting the Batch Size using a Data Loader\n"]},{"cell_type":"code","execution_count":26,"id":"312944b3-ed5a-4df4-a414-a362d1900c62","metadata":{},"outputs":[],"source":["batch_size=10"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<code> trainloader</code> = DataLoader(dataset=data_set, batch_size=10): Creates a DataLoader object called trainloader.It takes the data_set as the dataset argument and sets the batch_size to 10(no of samples taken for each iteration). The DataLoader is responsible for loading the data from the dataset in batches for efficient processing.\n","One epoch is the complete run on the dataset. "]},{"cell_type":"code","execution_count":27,"id":"32bc1486-e787-4312-8c7b-1edee1017e9b","metadata":{},"outputs":[],"source":["trainloader = DataLoader(dataset = data_set, batch_size = 10)  #it will output a batch of data "]},{"cell_type":"code","execution_count":28,"id":"3d28552e-c3e6-4da0-acf0-fbc03bda68d6","metadata":{},"outputs":[],"source":["dataset_iter = iter(trainloader)\n","#dataset_iter = iter(trainloader): Creates an iterator dataset_iter from the trainloader. It allows us to iterate over the batches of data in the trainloader dataset.\n"]},{"cell_type":"code","execution_count":29,"id":"b2581c8e-36f7-4e88-bcb8-8438424ab7cd","metadata":{},"outputs":[],"source":["X,y=next(dataset_iter )\n","#X, y = next(dataset_iter): Retrieves the next batch of data from the iterator using the next() function. It returns a tuple (X, y) containing the input features (X) and their corresponding labels (y) for the batch."]},{"cell_type":"markdown","id":"4703068b-ad53-4ede-9b0d-cf130eaaba22","metadata":{},"source":["We can see here that 10 values the same as our batch size\n"]},{"cell_type":"code","execution_count":null,"id":"3a88f76c-cfac-494e-bb93-a556cb42e577","metadata":{},"outputs":[],"source":["X"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y"]},{"cell_type":"markdown","id":"fb0f1591-b38e-4e0d-99bf-ccfb7110d432","metadata":{},"source":["## Setting the Learning Rate\n"]},{"cell_type":"markdown","id":"eace11ac-0a09-477e-a73b-38df815bcb85","metadata":{},"source":["We can set the learning rate by setting it as a parameter in the optimizer along with the parameters of the logistic regression model we are training. The job of the optimizer, torch.optim.SGD, is to use the loss generated by the criterion to update the model parameters according to the learning rate. SGD stands for Stochastic Gradient Descent which typically means that the batch size is set to 1, but the data loader we set up above has turned this into Mini-Batch Gradient Descent.\n"]},{"cell_type":"code","execution_count":32,"id":"04d6d90f-ea8a-4003-895f-a272be836b68","metadata":{},"outputs":[],"source":["learning_rate = 0.1"]},{"cell_type":"code","execution_count":62,"id":"7ebe130e-10d2-412a-8daf-721dd39b227e","metadata":{},"outputs":[],"source":["#torch.optim.SGD(...): Creates an optimizer object of the stochastic gradient descent (SGD) algorithm. It takes the parameters of the model and the learning rate (lr) as arguments. The optimizer will be responsible for updating the model's parameters based on the computed gradients during the training process.\n","optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"]},{"cell_type":"markdown","id":"feee3930-d81d-4b0b-8e8e-6e61fbe5fd55","metadata":{},"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","id":"f2b656bb-feee-4247-995d-5e50f629456b","metadata":{},"source":["<h2 id=\"BGD\">Train the Model via Mini-Batch Gradient Descent</h2>\n"]},{"cell_type":"markdown","id":"0c0bf957-9fdb-4a39-bca3-f9f60e2dcac8","metadata":{},"source":["We are going to train the model using various Batch Sizes and Learning Rates.\n"]},{"cell_type":"markdown","id":"fed0acaa-6bfc-4065-bd57-9a4d8080d1c4","metadata":{},"source":["### Mini-Batch Gradient Descent\n"]},{"attachments":{},"cell_type":"markdown","id":"97313801-1ae4-4714-b7af-3c2eab16ffae","metadata":{},"source":["In this case, we will set the batch size of the data loader to 5 and the number of epochs to 250.\n","\n","number of epochs: number of times the entire training dataset is passed.\n"]},{"cell_type":"markdown","id":"af875dd0-7a42-4cdc-a464-71021258b412","metadata":{},"source":["First, we must recreate the get_surface object again so that for each example we get a Loss Surface for that model only.\n"]},{"cell_type":"code","execution_count":null,"id":"8a90e75a-2f76-47e5-9140-077481ac4750","metadata":{},"outputs":[],"source":["get_surface = plot_error_surfaces(15, 13, data_set[:][0], data_set[:][1], 30)"]},{"cell_type":"markdown","id":"047306e8-ccc6-46a2-856f-526e5fcc7169","metadata":{},"source":["#### Train the Model\n"]},{"cell_type":"code","execution_count":null,"id":"acecb4c2-6c4e-4663-a0f1-081087fd18f9","metadata":{},"outputs":[],"source":["# First we create an instance of the model we want to train\n","model = logistic_regression(1)\n","# We create a criterion which will measure loss  using actual y and yhat.\n","criterion = nn.BCELoss()\n","# We create a data loader with the dataset and specified batch size of 5\n","trainloader = DataLoader(dataset = data_set, batch_size = 5)\n","# We create an optimizer with the model parameters and learning rate\n","optimizer = torch.optim.SGD(model.parameters(), lr = .01)\n","# Then we set the number of epochs which is the total number of times we will train on the entire training dataset\n","epochs=500\n","# This will store the loss over iterations so we can plot it at the end\n","loss_values = []  #stores loss for each batch.\n","\n","# Loop will execute for number of epochs\n","for epoch in range(epochs):\n","    # For each batch in the training data\n","    for x, y in trainloader:\n","        # Make our predictions from the X values\n","        yhat = model(x)   # yhat values based on x values.\n","        # Measure the loss between our prediction and actual Y values\n","        loss = criterion(yhat, y)   # loss corresponding to this batch\n","        # Resets the calculated gradient value, this must be done each time as it accumulates if we do not reset\n","        optimizer.zero_grad()\n","        # Calculates the gradient value with respect to each weight and bias\n","        loss.backward()\n","        # Updates the weight and bias according to calculated gradient value\n","        optimizer.step()\n","        # Set the parameters for the loss surface contour graphs\n","        get_surface.set_para_loss(model, loss.tolist())     #It collects the model's weight and bias values (model.parameters()) and the loss value (loss.tolist()) for visualization purposes.\n","        # Saves the loss of the iteration\n","        loss_values.append(loss)\n","    # Want to print the Data Space for the current iteration every 20 epochs\n","    if epoch % 20 == 0:\n","        get_surface.plot_ps()"]},{"cell_type":"markdown","id":"c0ef6580-09df-4568-bfd8-eabe030d7242","metadata":{},"source":["We can see the final values of the weight and bias. This weight and bias correspond to the orange line in the Data Space graph and the final spot of the X in the Loss Surface Contour graph.\n"]},{"cell_type":"code","execution_count":77,"id":"343f8d58-2f67-435c-bc0e-ce887b0e2c39","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["w =  tensor([2.7497]) b =  tensor(-0.7446)\n"]}],"source":["w = model.state_dict()['linear.weight'].data[0]\n","b = model.state_dict()['linear.bias'].data[0]\n","print(\"w = \", w, \"b = \", b)"]},{"attachments":{},"cell_type":"markdown","id":"165a85b3-cb05-4cbd-b01a-7983230f9abd","metadata":{},"source":["Now we can get the accuracy of the training data. We trained the model , now lets check the accuracy.\n"]},{"cell_type":"code","execution_count":78,"id":"8794285e-18ef-4ad1-be59-6e33f2988409","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy:  100.0 %\n"]}],"source":["# Getting the predictions\n","yhat = model(data_set.x)\n","# Rounding the prediction to the nearedt integer 0 or 1 representing the classes\n","yhat = torch.round(yhat)\n","# Counter to keep track of correct predictions\n","correct = 0\n","# Goes through each prediction and actual y value\n","for prediction, actual in zip(yhat, data_set.y):\n","    # Compares if the prediction and actualy y value are the same\n","    if (prediction == actual):\n","        # Adds to counter if prediction is correct\n","        correct+=1\n","# Outputs the accuracy by dividing the correct predictions by the length of the dataset\n","print(\"Accuracy: \", correct/len(data_set)*100, \"%\")"]},{"cell_type":"markdown","id":"62f6c9b2-041c-4ab4-a4e2-97d0f813b705","metadata":{},"source":["Finally, we plot the Cost vs Iteration graph, although it is erratic it is downward sloping.\n"]},{"cell_type":"code","execution_count":null,"id":"8330580e-0475-4112-b31a-d62c694e072a","metadata":{},"outputs":[],"source":["plt.plot(loss_values)\n","plt.xlabel(\"Iteration\")\n","plt.ylabel(\"Cost\")"]},{"cell_type":"markdown","id":"abe7182e-9e59-44f0-b0ae-d918c4f87ae2","metadata":{},"source":["### Stochastic Gradient Descent\n"]},{"cell_type":"markdown","id":"b771fcd9-ffff-449b-b741-8cdba6dbfeeb","metadata":{},"source":["In this case, we will set the batch size of the data loder to 1 so that the gradient descent will be performed for each example this is referred to as Stochastic Gradient Descent. The number of epochs is set to 100.\n","\n","Notice that in this example the batch size is decreased from 5 to 1 so there would be more iterations. Due to this, we can reduce the number of iterations by decreasing the number of epochs. Due to the reduced batch size, we are optimizing more frequently so we don't need as many epochs.\n","\n","First, we must recreate the `get_surface` object again so that for each example we get a Loss Surface for that model only.\n"]},{"cell_type":"code","execution_count":null,"id":"8e0b6552-1999-4714-b717-88e6aa6e4e87","metadata":{},"outputs":[],"source":["get_surface = plot_error_surfaces(15, 13, data_set[:][0], data_set[:][1], 30)"]},{"cell_type":"markdown","id":"a6493c2e-069e-433d-b24d-2127c1a1b670","metadata":{},"source":["#### Train the Model\n"]},{"cell_type":"code","execution_count":null,"id":"81072cad-5c03-440b-9b73-8dcf9b44def6","metadata":{},"outputs":[],"source":["# First we create an instance of the model we want to train\n","model = logistic_regression(1)\n","# We create a criterion which will measure loss\n","criterion = nn.BCELoss()\n","# We create a data loader with the dataset and specified batch size of 1\n","trainloader = DataLoader(dataset = data_set, batch_size = 1)\n","# We create an optimizer with the model parameters and learning rate\n","optimizer = torch.optim.SGD(model.parameters(), lr = .01)\n","# Then we set the number of epochs which is the total number of times we will train on the entire training dataset\n","epochs=100\n","# This will store the loss over iterations so we can plot it at the end\n","loss_values = []\n","\n","# Loop will execute for number of epochs\n","for epoch in range(epochs):\n","    # For each batch in the training data\n","    for x, y in trainloader:\n","        # Make our predictions from the X values\n","        yhat = model(x)\n","        # Measure the loss between our prediction and actual Y values\n","        loss = criterion(yhat, y)\n","        # Resets the calculated gradient value, this must be done each time as it accumulates if we do not reset\n","        optimizer.zero_grad()\n","        # Calculates the gradient value with respect to each weight and bias\n","        loss.backward()\n","        # Updates the weight and bias according to calculated gradient value\n","        optimizer.step()\n","        # Set the parameters for the loss surface contour graphs\n","        get_surface.set_para_loss(model, loss.tolist())\n","        # Saves the loss of the iteration\n","        loss_values.append(loss)\n","    # Want to print the Data Space for the current iteration every 20 epochs\n","    if epoch % 20 == 0:\n","        get_surface.plot_ps()"]},{"cell_type":"markdown","id":"9aaeac6a-d0f3-4551-a20f-b6beaf2faae9","metadata":{},"source":["We can see the final values of the weight and bias. This weight and bias correspond to the orange line in the Data Space graph and the final spot of the X in the Loss Surface Contour graph.\n"]},{"cell_type":"code","execution_count":82,"id":"c0a5e50c-2398-4f6c-81f0-56e9d7060553","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["w =  tensor([2.4882]) b =  tensor(-0.7023)\n"]}],"source":["w = model.state_dict()['linear.weight'].data[0]\n","b = model.state_dict()['linear.bias'].data[0]\n","print(\"w = \", w, \"b = \", b)"]},{"cell_type":"markdown","id":"9e8826d5-64e0-4cf4-bdd1-749e703c366e","metadata":{},"source":["Now we can get the accuracy of the training data\n"]},{"cell_type":"code","execution_count":83,"id":"12af51b1-7ed0-4927-b56d-8ab331083b43","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy:  100.0 %\n"]}],"source":["# Getting the predictions\n","yhat = model(data_set.x)\n","# Rounding the prediction to the nearedt integer 0 or 1 representing the classes\n","yhat = torch.round(yhat)\n","# Counter to keep track of correct predictions\n","correct = 0\n","# Goes through each prediction and actual y value\n","for prediction, actual in zip(yhat, data_set.y):\n","    # Compares if the prediction and actualy y value are the same\n","    if (prediction == actual):\n","        # Adds to counter if prediction is correct\n","        correct+=1\n","# Outputs the accuracy by dividing the correct predictions by the length of the dataset\n","print(\"Accuracy: \", correct/len(data_set)*100, \"%\")"]},{"cell_type":"markdown","id":"4e6f3765-7e26-43db-82f4-8f6294161044","metadata":{},"source":["Finally, we plot the Cost vs Iteration graph, although it is erratic it is downward sloping.\n"]},{"cell_type":"code","execution_count":null,"id":"a61330c6-d52a-450b-9fc8-89aa78c68bb9","metadata":{},"outputs":[],"source":["plt.plot(loss_values)\n","plt.xlabel(\"Iteration\")\n","plt.ylabel(\"Cost\")"]},{"cell_type":"markdown","id":"6e45f6c8-15fa-4289-90aa-6a0dc33be66d","metadata":{},"source":["### High Learning Rate\n"]},{"cell_type":"markdown","id":"4b11a984-0b64-49c3-9321-44f03de10d08","metadata":{},"source":["In this case, we will set the batch size of the data loder to 1 so that the gradient descent will be performed for each example this is referred to as Stochastic Gradient Descent. This time the learning rate will be set to .1 to represent a high learning rate and we will observe what will happen when we try to train.\n","\n","First, we must recreate the `get_surface` object again so that for each example we get a Loss Surface for that model only.\n"]},{"cell_type":"code","execution_count":null,"id":"c2d29fa1-5fc0-4ea9-b094-8d26e6a2357f","metadata":{},"outputs":[],"source":["get_surface = plot_error_surfaces(15, 13, data_set[:][0], data_set[:][1], 30)"]},{"cell_type":"markdown","id":"f2554fd0-6ed2-46e5-b4f9-f6ddacd58b87","metadata":{},"source":["#### Train the Model\n"]},{"cell_type":"code","execution_count":null,"id":"a9333aac-9301-4759-ac86-33298ef52844","metadata":{},"outputs":[],"source":["# First we create an instance of the model we want to train\n","model = logistic_regression(1)\n","# We create a criterion that will measure loss\n","criterion = nn.BCELoss()\n","# We create a data loader with the dataset and specified batch size of 1\n","trainloader = DataLoader(dataset = data_set, batch_size = 1)\n","# We create an optimizer with the model parameters and learning rate\n","optimizer = torch.optim.SGD(model.parameters(), lr = 1)\n","# Then we set the number of epochs which is the total number of times we will train on the entire training dataset\n","epochs=100\n","# This will store the loss over iterations so we can plot it at the end\n","loss_values = []\n","\n","# Loop will execute for number of epochs\n","for epoch in range(epochs):\n","    # For each batch in the training data\n","    for x, y in trainloader:\n","        # Make our predictions from the X values\n","        yhat = model(x)\n","        # Measure the loss between our prediction and actual Y values\n","        loss = criterion(yhat, y)\n","        # Resets the calculated gradient value, this must be done each time as it accumulates if we do not reset\n","        optimizer.zero_grad()\n","        # Calculates the gradient value with respect to each weight and bias\n","        loss.backward()\n","        # Updates the weight and bias according to calculated gradient value\n","        optimizer.step()\n","        # Set the parameters for the loss surface contour graphs\n","        get_surface.set_para_loss(model, loss.tolist())\n","        # Saves the loss of the iteration\n","        loss_values.append(loss)\n","    # Want to print the Data Space for the current iteration every 20 epochs\n","    if epoch % 20 == 0:\n","        get_surface.plot_ps()"]},{"cell_type":"markdown","id":"4e96acd8-93a4-4259-8ba6-8a905a4f87c1","metadata":{},"source":["Notice in this example the due to the high learning rate the Loss Surface Contour graph has increased movement over the previous example and also moves in multiple directions due to the minimum being overshot.\n"]},{"cell_type":"markdown","id":"8290f3d6-5760-4f62-97e6-e78f7852872c","metadata":{},"source":["We can see the final values of the weight and bias. This weight and bias correspond to the orange line in the Data Space graph and the final spot of the X in the Loss Surface Contour graph.\n"]},{"cell_type":"code","execution_count":null,"id":"e1ae2c60-d2ca-4ffc-aea6-ccc1ea5215c8","metadata":{},"outputs":[],"source":["w = model.state_dict()['linear.weight'].data[0]\n","b = model.state_dict()['linear.bias'].data[0]\n","print(\"w = \", w, \"b = \", b)"]},{"cell_type":"markdown","id":"97a08fbb-1cb2-4539-b3e5-c5b67d455f21","metadata":{},"source":["Now we can get the accuracy of the training data\n"]},{"cell_type":"code","execution_count":null,"id":"da3d5ded-d719-4779-8396-47565e3035b1","metadata":{},"outputs":[],"source":["# Getting the predictions\n","yhat = model(data_set.x)\n","# Rounding the prediction to the nearedt integer 0 or 1 representing the classes\n","yhat = torch.round(yhat)\n","# Counter to keep track of correct predictions\n","correct = 0\n","# Goes through each prediction and actual y value\n","for prediction, actual in zip(yhat, data_set.y):\n","    # Compares if the prediction and actualy y value are the same\n","    if (prediction == actual):\n","        # Adds to counter if prediction is correct\n","        correct+=1\n","# Outputs the accuracy by dividing the correct predictions by the length of the dataset\n","print(\"Accuracy: \", correct/len(data_set)*100, \"%\")"]},{"cell_type":"markdown","id":"1c2ee3d2-4576-47c8-95a3-bdee2cd7d9ac","metadata":{},"source":["Finally, we plot the Cost vs Iteration graph, although it is erratic it is downward sloping.\n"]},{"cell_type":"code","execution_count":null,"id":"f1600257-4933-404e-9b50-50a4e79cbe68","metadata":{},"outputs":[],"source":["plt.plot(loss_values)\n","plt.xlabel(\"Iteration\")\n","plt.ylabel(\"Cost\")"]},{"cell_type":"markdown","id":"ed508576-4208-4475-800f-8d37a2b809e4","metadata":{},"source":["## Question\n"]},{"cell_type":"markdown","id":"3b0fb361-01bd-465e-9708-6aa92ebd5ed6","metadata":{},"source":["Using the following code train the model using a `learning rate` of `.01`, `120 epochs`, and `batch_size` of `1`.\n"]},{"cell_type":"code","execution_count":null,"id":"317d86cc-337e-405b-9c8d-97498f676c5b","metadata":{},"outputs":[],"source":["get_surface = plot_error_surfaces(15, 13, data_set[:][0], data_set[:][1], 30)"]},{"cell_type":"markdown","id":"b0b88d56-1088-46e9-b215-901b3fe567f2","metadata":{},"source":["#### Train the Model\n"]},{"cell_type":"code","execution_count":null,"id":"2c4972ab-a538-4ed4-bdb4-22de67a13639","metadata":{},"outputs":[],"source":["# First we create an instance of the model we want to train\n","model = logistic_regression(1)\n","# We create a criterion which will measure loss\n","criterion = nn.BCELoss()\n","# We create a data loader with the dataset and specified batch size of 1\n","trainloader = DataLoader(dataset = data_set, batch_size = \"SET_BATCH_SIZE\")\n","# We create an optimizer with the model parameters and learning rate\n","optimizer = torch.optim.SGD(model.parameters(), lr = \"SET_LEARNING_RATE\")\n","# Then we set the number of epochs which is the total number of times we will train on the entire training dataset\n","epochs= \"SET_NUMBER_OF_EPOCHS\"\n","# This will store the loss over iterations so we can plot it at the end\n","loss_values = []\n","\n","# Loop will execute for number of epochs\n","for epoch in range(epochs):\n","    # For each batch in the training data\n","    for x, y in trainloader:\n","        # Make our predictions from the X values\n","        yhat = model(x)\n","        # Measure the loss between our prediction and actual Y values\n","        loss = criterion(yhat, y)\n","        # Resets the calculated gradient value, this must be done each time as it accumulates if we do not reset\n","        optimizer.zero_grad()\n","        # Calculates the gradient value with respect to each weight and bias\n","        loss.backward()\n","        # Updates the weight and bias according to calculated gradient value\n","        optimizer.step()\n","        # Set the parameters for the loss surface contour graphs\n","        get_surface.set_para_loss(model, loss.tolist())\n","        # Saves the loss of the iteration\n","        loss_values.append(loss)\n","    # Want to print the Data Space for the current iteration every 20 epochs\n","    if epoch % 20 == 0:\n","        get_surface.plot_ps()"]},{"cell_type":"markdown","id":"a22ac469-6404-46e1-a756-288d17835012","metadata":{},"source":["<details><summary>Click here for the solution</summary>\n","<code>  \n","# First we create an instance of the model we want to train\n","model = logistic_regression(1)\n","# We create a criterion which will measure loss\n","criterion = nn.BCELoss()\n","# We create a data loader with the dataset and specified batch size of 1\n","trainloader = DataLoader(dataset = data_set, batch_size = 1)\n","# We create an optimizer with the model parameters and learning rate\n","optimizer = torch.optim.SGD(model.parameters(), lr = .01)\n","# Then we set the number of epochs which is the total number of times we will train on the entire training dataset\n","epochs= 120\n","# This will store the loss over iterations so we can plot it at the end\n","loss_values = []\n","# Loop will execute for number of epochs\n","for epoch in range(epochs):\n","    # For each batch in the training data\n","    for x, y in trainloader:\n","        # Make our predictions from the X values\n","        yhat = model(x)\n","        # Measure the loss between our prediction and actual Y values\n","        loss = criterion(yhat, y)\n","        # Resets the calculated gradient value, this must be done each time as it accumulates if we do not reset\n","        optimizer.zero_grad()\n","        # Calculates the gradient value with respect to each weight and bias\n","        loss.backward()\n","        # Updates the weight and bias according to calculated gradient value\n","        optimizer.step()\n","        # Set the parameters for the loss surface contour graphs\n","        get_surface.set_para_loss(model, loss.tolist())\n","        # Saves the loss of the iteration\n","        loss_values.append(loss)\n","    # Want to print the Data Space for the current iteration every 20 epochs\n","    if epoch % 20 == 0:\n","        get_surface.plot_ps()\n","</code>\n","</details>\n"]},{"cell_type":"markdown","id":"bfdb9ab3-bdec-4083-8e78-5d2e3cbf6b41","metadata":{},"source":["We can see the final values of the weight and bias. This weight and bias correspond to the orange line in the Data Space graph and the final spot of the X in the Loss Surface Contour graph.\n"]},{"cell_type":"code","execution_count":null,"id":"a7c57b49-6927-44d9-8cc6-4bc8ef7dbd92","metadata":{},"outputs":[],"source":["w = model.state_dict()['linear.weight'].data[0]\n","b = model.state_dict()['linear.bias'].data[0]\n","print(\"w = \", w, \"b = \", b)"]},{"cell_type":"markdown","id":"d793d2ac-a2ee-40f7-8c3c-2e61b6b6f349","metadata":{},"source":["Now we can get the accuracy of the training data\n"]},{"cell_type":"code","execution_count":null,"id":"e9bd7029-2046-42b1-a897-4fe8fde871ee","metadata":{},"outputs":[],"source":["# Getting the predictions\n","yhat = model(data_set.x)\n","# Rounding the prediction to the nearedt integer 0 or 1 representing the classes\n","yhat = torch.round(yhat)\n","# Counter to keep track of correct predictions\n","correct = 0\n","# Goes through each prediction and actual y value\n","for prediction, actual in zip(yhat, data_set.y):\n","    # Compares if the prediction and actualy y value are the same\n","    if (prediction == actual):\n","        # Adds to counter if prediction is correct\n","        correct+=1\n","# Outputs the accuracy by dividing the correct predictions by the length of the dataset\n","print(\"Accuracy: \", correct/len(data_set)*100, \"%\")"]},{"cell_type":"markdown","id":"9533cd85-0562-46ea-bef8-9a716284715b","metadata":{},"source":["Finally, we plot the Cost vs Iteration graph, although it is erratic it is downward sloping.\n"]},{"cell_type":"code","execution_count":null,"id":"9bd9a6d6-08f3-4b59-b6f1-6b3bd1d2b966","metadata":{},"outputs":[],"source":["plt.plot(loss_values)\n","plt.xlabel(\"Iteration\")\n","plt.ylabel(\"Cost\")"]},{"cell_type":"markdown","id":"7f509140-e246-412b-afab-ae6c8714c714","metadata":{},"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","id":"7019d50b-4ed3-4d58-a42e-91516f9c3f49","metadata":{},"source":["<hr>\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":4}
